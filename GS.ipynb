{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import array\n",
    "from keras.datasets import mnist\n",
    "import keras.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 60000\n",
      "Training data shape: (60000, 28, 28)\n",
      "Testing data: 10000\n",
      "Testing data shape: (10000, 28, 28)\n",
      "Train label: 60000\n",
      "Train label shape: (60000,)\n",
      "Train label data shape: (60000, 10)\n",
      "Test label data shape: (10000, 10)\n",
      "Test label: 10000\n",
      "Test label shape: (10000, 10)\n",
      "Train data first elements value:\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "Train label first elements label: 5\n"
     ]
    }
   ],
   "source": [
    "(train_data,train_label),(test_data,test_label) = mnist.load_data()\n",
    "print('Training data:',len(train_data))\n",
    "print('Training data shape:',train_data.shape)\n",
    "print('Testing data:',len(test_data))\n",
    "print('Testing data shape:',test_data.shape)\n",
    "print('Train label:', len(train_label))\n",
    "print('Train label shape:',train_label.shape)\n",
    "train_label_data = keras.utils.to_categorical(train_label, 10)\n",
    "test_label_data = keras.utils.to_categorical(test_label, 10)\n",
    "print('Train label data shape:',train_label_data.shape)\n",
    "print('Test label data shape:', test_label_data.shape)\n",
    "print('Test label:',len(test_label))\n",
    "print('Test label shape:',test_label_data.shape)\n",
    "print('Train data first elements value:')\n",
    "print(train_data[0])\n",
    "print('Train label first elements label:',train_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape: (60000, 28, 28)\n",
      "testing data shape: (10000, 28, 28)\n",
      "After\n",
      "----------------------------------\n",
      "training data shape: (60000, 784)\n",
      "testing data shape: (10000, 784)\n",
      "Changed train data:\n",
      " [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
      " 0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.\n",
      " 0.96862745 0.49803922 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
      " 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19215686\n",
      " 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863\n",
      " 0.32156863 0.21960784 0.15294118 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07058824 0.85882353 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549\n",
      " 0.96862745 0.94509804 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
      " 0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04313725\n",
      " 0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.1372549  0.94509804\n",
      " 0.88235294 0.62745098 0.42352941 0.00392157 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.31764706 0.94117647 0.99215686\n",
      " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
      " 0.58823529 0.10588235 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.97647059 0.99215686 0.97647059 0.25098039 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
      " 0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.15294118 0.58039216\n",
      " 0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.78823529 0.30588235 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07058824 0.67058824\n",
      " 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588\n",
      " 0.31372549 0.03529412 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53333333 0.99215686\n",
      " 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "After\n",
      "----------------------------------\n",
      "train_label: 5\n",
      "After\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_data_data = train_data.astype('float32')\n",
    "test_data_data = test_data.astype('float32')\n",
    "\n",
    "train_data = train_data / 255\n",
    "test_data = test_data / 255\n",
    "print('training data shape:',train_data.shape)\n",
    "print('testing data shape:',test_data.shape)\n",
    "train_data_data = train_data.reshape(60000,784)\n",
    "test_data_data = test_data.reshape(10000,784)\n",
    "\n",
    "# train_label_data = keras.utils.to_categorical(train_label, 10)\n",
    "# test_label_data = keras.utils.to_categorical(test_label, 10)\n",
    "\n",
    "print('After\\n----------------------------------')\n",
    "print('training data shape:',train_data_data.shape)\n",
    "print('testing data shape:',test_data_data.shape)\n",
    "print(\"Changed train data:\\n\",train_data_data[0])\n",
    "print('After\\n----------------------------------')\n",
    "print('train_label:',train_label[0])\n",
    "print('After\\n----------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ...\n",
      "Ehnii 100 -n loss bol: 2.325857916497158\n",
      "Ehnii 200 -n loss bol: 2.323257454724505\n",
      "Ehnii 300 -n loss bol: 120.68581366431616\n",
      "Ehnii 400 -n loss bol: nan\n",
      "Ehnii 500 -n loss bol: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: divide by zero encountered in log\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ehnii 600 -n loss bol: nan\n",
      "Ehnii 700 -n loss bol: nan\n",
      "Ehnii 800 -n loss bol: nan\n",
      "Ehnii 900 -n loss bol: nan\n",
      "Ehnii 1000 -n loss bol: nan\n",
      "all loss average is: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NN(object):\n",
    "    def __init__(self,input_data,hidden_layer_data,output_data):\n",
    "        self.input = input_data\n",
    "        self.hidden = hidden_layer_data\n",
    "        self.output = output_data\n",
    "        self.w1 = np.random.randn(input_data,hidden_layer_data)*0.01\n",
    "#         print('w1_shape:',self.w1.shape)\n",
    "        self.b1 = np.zeros((1,hidden_layer_data))\n",
    "        self.w2 = np.random.randn(hidden_layer_data,output_data)*0.01\n",
    "#         print('w2_shape:',self.w2.shape)\n",
    "        self.b2 = np.zeros((1,output_data))\n",
    "        print('Starting ...')\n",
    "        \n",
    "    def softmax(self,k):\n",
    "        e_x = np.exp(k - np.max(k))\n",
    "        return e_x / e_x.sum(axis=0) # only difference\n",
    "    \n",
    "    def relu(self,x):\n",
    "        return np.maximum(0,x)\n",
    "    \n",
    "    def output_formula(self, batch):\n",
    "        layer1 = np.add(batch@self.w1,self.b1)\n",
    "        layer1 = self.relu(layer1)\n",
    "        layer2 = np.add(layer1@self.w2,self.b2)\n",
    "        layer2 = self.softmax(layer2)\n",
    "        return layer1,self.w2,layer2\n",
    "    \n",
    "#     def error_formula(self,output,y):\n",
    "#         loss = 0\n",
    "#         for j in range(len(output)):\n",
    "#             logprobs = -np.log()\n",
    "    \n",
    "    def error_formula(self, output, y):\n",
    "        # output prediction\n",
    "        # y true label\n",
    "        loss = 0\n",
    "        loss1 = 0\n",
    "        div = len(y)\n",
    "        for j in range(div):\n",
    "            for i in range(self.output):\n",
    "#                 print('y',y[j][i])\n",
    "                logprobs = -(y[j][i]*np.log(output[j][i]))\n",
    "                loss = loss + logprobs\n",
    "            loss1 = loss1 + loss\n",
    "        loss1 = loss1/div\n",
    "        return loss1\n",
    "#         print('H shape:',H.shape)\n",
    "#         print('output shape:',output.shape)\n",
    "#         print('Y shape:',y.shape)\n",
    "#         print('batch shape:',batch.shape)\n",
    "#         print('w2 shape:',w2.shape)\n",
    "#         print('my shape is',d_error.shape)\n",
    "\n",
    "    def update_weights(self,batch,y,learnrate):\n",
    "        H,w2,output = self.output_formula(batch)\n",
    "        d_error = np.zeros((100,10))\n",
    "        for i in range(len(batch)):\n",
    "            for j in range(output_data):\n",
    "                d_error[i][j] = output[i][j]-y[i][j]\n",
    "            grad_w1 = (np.transpose(batch)@(d_error))@np.transpose(w2)*learnrate\n",
    "            grad_w2 = np.transpose(H)@(d_error)*learnrate\n",
    "        self.w1 = self.w1 - grad_w1\n",
    "        self.w2 = self.w2 - grad_w2\n",
    "#         return self.w1,self.w2\n",
    "    \n",
    "#         update weights deer print hiij bsan ymnuud.\n",
    "#         print('d_error shape:',d_error.shape)\n",
    "#         print('grad_w2 shape:',grad_w2.shape)\n",
    "#         print('grad_w1 shape:',grad_w1.shape)\n",
    "#         print('w1 shape:',w1.shape)\n",
    "#         print('w2 shape:',w2.shape)\n",
    "\n",
    "    def train(self,input_list,target_list,batch_size,learnrate):\n",
    "        all_loss = 0\n",
    "        for i in range(0,1000,batch_size):\n",
    "            batch = input_list[i:min(i+batch_size,input_list.shape[0]),:]\n",
    "            batch_label = target_list[i:min(i+batch_size,target_list.shape[0]),:]\n",
    "            Layer1,w_1,output = self.output_formula(batch)\n",
    "            loss = self.error_formula(output,batch_label)\n",
    "            self.update_weights(batch,batch_label,learnrate)\n",
    "            avg_loss = loss/batch_size\n",
    "            print('Ehnii',i+100,'-n loss bol:',avg_loss)\n",
    "            all_loss = all_loss + avg_loss\n",
    "        wak = 1000/batch_size\n",
    "        all_loss = all_loss/wak\n",
    "        print('all loss average is:',all_loss)\n",
    "        return all_loss\n",
    "\n",
    "input_data = 784\n",
    "hidden_layer_data = 32\n",
    "output_data = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "object1 = NN(input_data,hidden_layer_data,output_data)\n",
    "object1.train(train_data_data,train_label_data,batch_size,learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
